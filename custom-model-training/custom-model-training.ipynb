{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassifier(nn.Module):\n",
    "  def __init__(self, lstm_input_size, lstm_hidden_size, lstm_num_layers, num_classes):\n",
    "    super(VideoClassifier, self).__init__()\n",
    "    self.mobilenet = models.mobilenet_v2(weights='DEFAULT')\n",
    "    self.lstm_input_size = lstm_input_size\n",
    "    self.lstm_hidden_size = lstm_hidden_size\n",
    "    self.lstm_num_layers = lstm_num_layers\n",
    "\n",
    "    # Freeze MobileNetV2 layers so they don't get trained.\n",
    "    for param in self.mobilenet.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    # making last layer identity(output = input), effectively making last layer numb\n",
    "    self.mobilenet.classifier = nn.Identity()\n",
    "\n",
    "    # making lstm network\n",
    "    self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size,\n",
    "                        lstm_num_layers, batch_first=True, dropout=0.2)\n",
    "\n",
    "    # making FC layer for binary prediction\n",
    "    self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # pass x through mobilenet to extract features\n",
    "    features = self.mobilenet(x)\n",
    "\n",
    "    # reshaping features for lstm input\n",
    "    features = features.view(x.size(0), -1, features.size(1))\n",
    "\n",
    "    # passing through lstm layers\n",
    "    lstm_out, _ = self.lstm(features)\n",
    "\n",
    "    # tooking output from last time step\n",
    "    lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "    # passing through fc to get final output\n",
    "    output = self.fc(lstm_out)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to C:\\Users\\k26ra/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-7ebf99e0.pth\n",
      "100%|██████████| 13.6M/13.6M [00:48<00:00, 295kB/s] \n"
     ]
    }
   ],
   "source": [
    "input_size = 1280  # Output size of MobileNetV2\n",
    "hidden_size = 256  # Size of hidden state in LSTM\n",
    "# Number of layers in LSTM (dropout expects more than 1 layers)\n",
    "num_layers = 2\n",
    "num_classes = 2    # Binary prediction\n",
    "\n",
    "model = VideoClassifier(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories1 = ['violent', 'non-violent']\n",
    "categories2 = ['cam1', 'cam2']\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for category in categories1:\n",
    "  for typex in categories2:\n",
    "\n",
    "    path = os.path.join('Dataset', category, typex)\n",
    "    label = categories1.index(category)\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "      videos = os.path.join(path, file)\n",
    "\n",
    "      data.append([videos, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Dataset\\\\violent\\\\cam1\\\\47.mp4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for features, label in data:\n",
    "  x.append(features)\n",
    "  y.append(label)\n",
    "\n",
    "len(x)\n",
    "len(y)\n",
    "x[0]\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = ['video1.mp4', 'video2.mp4']  # List of video file paths\n",
    "# y = [1, 0]  # Corresponding labels (0 for violence, 1 for non-violence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Dataset\\\\non-violent\\\\cam1\\\\40.mp4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)\n",
    "len(y_train)\n",
    "x_train[90]\n",
    "y_train[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "  def __init__(self, video_paths, labels, max_frame, transform=None):\n",
    "    self.video_paths = video_paths\n",
    "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    self.max_frame = max_frame\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.video_paths)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    video_path = self.video_paths[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    # read video_frames for each vid\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        i += 1\n",
    "        print(f'finished reading frames of video : {i}')\n",
    "        break\n",
    "      # converting color from bgr to rgb\n",
    "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Truncate or pad frames to ensure fixed length\n",
    "    if len(frames) < self.max_frame:\n",
    "      # Pad frames with zeros\n",
    "      frames += [np.zeros_like(frames[0])] * (self.max_frame - len(frames))\n",
    "    elif len(frames) > self.max_frame:\n",
    "      # Truncate frames\n",
    "      frames = frames[:self.max_frame]\n",
    "\n",
    "    # applying tranform\n",
    "    if self.transform:\n",
    "      frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "    # Stack frames into a tensor\n",
    "    frames_tensor = torch.stack(frames)\n",
    "\n",
    "    return frames_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frame = 300\n",
    "\n",
    "train_dataset = VideoDataset(x_train, y_train, max_frame, transform=transform)\n",
    "test_dataset = VideoDataset(x_test, y_test, max_frame, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.lstm.parameters()},\n",
    "    {'params': model.fc.parameters()}\n",
    "], lr=1e-3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4325634\n",
      "2101762\n"
     ]
    }
   ],
   "source": [
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel()\n",
    "                       for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (frames, labels) in enumerate(train_dataloader):\n",
    "#   print(frames.shape)\n",
    "#   print(labels.shape)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight file exists. Skipping training.\n",
      "Model weights loaded from existing file.\n"
     ]
    }
   ],
   "source": [
    "force_train = False\n",
    "num_epochs = 2\n",
    "start_epoch = num_epochs\n",
    "\n",
    "\n",
    "# Check if model_weight.pth exists\n",
    "model_weight_file = 'model_weight.pth'\n",
    "\n",
    "if os.path.exists(model_weight_file):\n",
    "  model.load_state_dict(torch.load(model_weight_file))\n",
    "  # If model_weight.pth exists and no force flag is set, skip training\n",
    "  if not force_train:\n",
    "    print(\"Model weight file exists. Skipping training.\")\n",
    "    print(\"Model weights loaded from existing file.\")\n",
    "  else:\n",
    "    print(\"Previous model_weight.pth file found.\")\n",
    "    print(\"Continuing training from previous state.\")\n",
    "    start_epoch = 0\n",
    "else:\n",
    "  # If model_weight.pth doesn't exist, start training\n",
    "  print(\"No previous model_weight.pth file found. Starting training.\")\n",
    "  start_epoch = 0\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "  model.train()\n",
    "  train_loss = 0.0\n",
    "  for batch_idx, (frames, labels) in enumerate(train_dataloader):\n",
    "\n",
    "    # reshaping features for model input\n",
    "    batch_size, num_frames, channels, height, width = frames.size()\n",
    "    reshaped_frames = frames.view(\n",
    "        batch_size * num_frames, channels, height, width)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(reshaped_frames)\n",
    "\n",
    "    # Reshape output to split batch and frame dimensions\n",
    "    reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "    probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "    # Aggregate predictions for each video\n",
    "    aggregated_probabilities = probabilities.mean(dim=1)\n",
    "    # Get the predicted class for each video\n",
    "    _, predicted_classes = torch.max(aggregated_probabilities, dim=1)\n",
    "\n",
    "    loss = criterion(aggregated_probabilities, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  # Validation loop\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (frames, labels) in enumerate(test_dataloader):\n",
    "\n",
    "      # reshaping features for model input\n",
    "      batch_size, num_frames, channels, height, width = frames.size()\n",
    "      reshaped_frames = frames.view(\n",
    "          batch_size * num_frames, channels, height, width)\n",
    "\n",
    "      outputs = model(reshaped_frames)\n",
    "\n",
    "      # Reshape output to split batch and frame dimensions\n",
    "      reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "      probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "      # Aggregate predictions for each video\n",
    "      aggregated_probabilities = probabilities.mean(dim=1)\n",
    "      # Get the predicted class for each video\n",
    "      _, predicted_classes = torch.max(aggregated_probabilities, dim=1)\n",
    "\n",
    "      loss = criterion(aggregated_probabilities, labels)\n",
    "\n",
    "      val_loss += loss.item()\n",
    "      # _, predicted = torch.max(outputs, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted_classes == labels).sum().item()\n",
    "\n",
    "  # Calculate validation accuracy\n",
    "  val_loss /= len(test_dataloader.dataset)\n",
    "  val_accuracy = 100. * correct / total\n",
    "\n",
    "  # Print validation results\n",
    "  print(f\"Validation Results - Epoch {epoch+1}:\")\n",
    "  print(f\"Validation Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "  # Save training and validation losses\n",
    "  train_losses.append(train_loss/len(train_dataloader.dataset))\n",
    "  val_losses.append(val_loss)\n",
    "\n",
    "  # Save model weights after every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    torch.save(model.state_dict(), model_weight_file)\n",
    "    print(f\"Epoch {epoch+1}: Model weights saved as {model_weight_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the video frames\n",
    "def preprocess_frame(frame):\n",
    "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "  frame = transform(frame)\n",
    "  return frame\n",
    "\n",
    "# Function to classify video\n",
    "\n",
    "\n",
    "def classify_video(video_path, threshold=0.5):\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "  frames = []\n",
    "  while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "      break\n",
    "    frame = preprocess_frame(frame)\n",
    "    frames.append(frame)\n",
    "\n",
    "  cap.release()\n",
    "\n",
    "  # Convert frames to tensor and add batch dimension\n",
    "  frames_tensor = torch.stack(frames).unsqueeze(0)\n",
    "\n",
    "  # Pass frames through the model\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    # Reshape frames for model input\n",
    "    batch_size, num_frames, channels, height, width = frames_tensor.size()\n",
    "    reshaped_frames = frames_tensor.view(\n",
    "        batch_size * num_frames, channels, height, width)\n",
    "\n",
    "    # Get model outputs\n",
    "    outputs = model(reshaped_frames)\n",
    "\n",
    "    # Reshape output to split batch and frame dimensions\n",
    "    reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "    probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "\n",
    "    # Calculate the overall probability of violence across all frames\n",
    "    overall_probability = probabilities[:, :, 1].mean()*100\n",
    "\n",
    "    # Determine if the overall probability exceeds the threshold\n",
    "    if overall_probability > threshold:\n",
    "      prediction = \"Violence\"\n",
    "    else:\n",
    "      prediction = \"Non-violence\"\n",
    "\n",
    "    return overall_probability.item(), prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: (0.4308839738368988, 'Non-violence')\n"
     ]
    }
   ],
   "source": [
    "video_path = r\"Dataset\\violent\\cam1\\47.mp4\"  # Path to your test video\n",
    "result = classify_video(video_path)\n",
    "print(\"Classification:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
