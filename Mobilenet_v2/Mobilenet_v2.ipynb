{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoClassifier(nn.Module):\n",
    "  def __init__(self, lstm_input_size, lstm_hidden_size, lstm_num_layers, num_classes):\n",
    "    super(VideoClassifier, self).__init__()\n",
    "    self.mobilenet = models.mobilenet_v2(weights='DEFAULT')\n",
    "    self.lstm_input_size = lstm_input_size\n",
    "    self.lstm_hidden_size = lstm_hidden_size\n",
    "    self.lstm_num_layers = lstm_num_layers\n",
    "\n",
    "    # Freeze MobileNetV2 layers so they don't get trained.\n",
    "    for param in self.mobilenet.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    # making last layer identity(output = input), effectively making last layer numb\n",
    "    self.mobilenet.classifier = nn.Identity()\n",
    "\n",
    "    # making lstm network\n",
    "    self.lstm = nn.LSTM(lstm_input_size, lstm_hidden_size,\n",
    "                        lstm_num_layers, batch_first=True, dropout=0.2)\n",
    "\n",
    "    # making FC layer for binary prediction\n",
    "    self.fc = nn.Linear(lstm_hidden_size, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # pass x through mobilenet to extract features\n",
    "    features = self.mobilenet(x)\n",
    "\n",
    "    # reshaping features for lstm input\n",
    "    features = features.view(x.size(0), -1, features.size(1))\n",
    "\n",
    "    # passing through lstm layers\n",
    "    lstm_out, _ = self.lstm(features)\n",
    "\n",
    "    # tooking output from last time step\n",
    "    lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "    # passing through fc to get final output\n",
    "    output = self.fc(lstm_out)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1280  # Output size of MobileNetV2\n",
    "hidden_size = 256  # Size of hidden state in LSTM\n",
    "# Number of layers in LSTM (dropout expects more than 1 layers)\n",
    "num_layers = 2\n",
    "num_classes = 2    # Binary prediction\n",
    "\n",
    "model = VideoClassifier(input_size, hidden_size,\n",
    "                        num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories1 = ['violent', 'non-violent']\n",
    "categories2 = ['cam1', 'cam2']\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "for category in categories1:\n",
    "  for typex in categories2:\n",
    "\n",
    "    path = os.path.join('Dataset', category, typex)\n",
    "    label = categories1.index(category)\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "      videos = os.path.join(path, file)\n",
    "\n",
    "      data.append([videos, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for features, label in data:\n",
    "  x.append(features)\n",
    "  y.append(label)\n",
    "\n",
    "len(x)\n",
    "len(y)\n",
    "x[0]\n",
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = ['video1.mp4', 'video2.mp4']  # List of video file paths\n",
    "# y = [1, 0]  # Corresponding labels (0 for violence, 1 for non-violence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train)\n",
    "len(y_train)\n",
    "# x_train[90]\n",
    "# y_train[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "  def __init__(self, video_paths, labels, max_frame, transform=None):\n",
    "    self.video_paths = video_paths\n",
    "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    self.max_frame = max_frame\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.video_paths)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    video_path = self.video_paths[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    # read video_frames for each vid\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        i += 1\n",
    "        print(f'finished reading frames of video : {i}')\n",
    "        break\n",
    "      # converting color from bgr to rgb\n",
    "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "      frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Truncate or pad frames to ensure fixed length\n",
    "    if len(frames) < self.max_frame:\n",
    "      # Pad frames with zeros\n",
    "      frames += [np.zeros_like(frames[0])] * (self.max_frame - len(frames))\n",
    "    elif len(frames) > self.max_frame:\n",
    "      # Truncate frames\n",
    "      frames = frames[:self.max_frame]\n",
    "\n",
    "    # applying tranform\n",
    "    if self.transform:\n",
    "      frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "    # # Stack frames into a tensor\n",
    "    # frames_tensor = torch.stack(frames)\n",
    "\n",
    "    # Convert frames to tensor and move to GPU\n",
    "    frames_tensor = torch.stack(frames).to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    return frames_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frame = 300\n",
    "\n",
    "train_dataset = VideoDataset(x_train, y_train, max_frame, transform=transform)\n",
    "test_dataset = VideoDataset(x_test, y_test, max_frame, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.lstm.parameters()},\n",
    "    {'params': model.fc.parameters()}\n",
    "], lr=1e-3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel()\n",
    "                       for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch_idx, (frames, labels) in enumerate(train_dataloader):\n",
    "#   print(frames.shape)\n",
    "#   print(labels.shape)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_train = False\n",
    "num_epochs = 2\n",
    "start_epoch = num_epochs\n",
    "\n",
    "\n",
    "# Check if model_weight.pth exists\n",
    "model_weight_file = 'model_weight.pth'\n",
    "\n",
    "if os.path.exists(model_weight_file):\n",
    "  model.load_state_dict(torch.load(model_weight_file))\n",
    "  # If model_weight.pth exists and no force flag is set, skip training\n",
    "  if not force_train:\n",
    "    print(\"Model weight file exists. Skipping training.\")\n",
    "    print(\"Model weights loaded from existing file.\")\n",
    "  else:\n",
    "    print(\"Previous model_weight.pth file found.\")\n",
    "    print(\"Continuing training from previous state.\")\n",
    "    start_epoch = 0\n",
    "else:\n",
    "  # If model_weight.pth doesn't exist, start training\n",
    "  print(\"No previous model_weight.pth file found. Starting training.\")\n",
    "  start_epoch = 0\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "  model.train()\n",
    "  train_loss = 0.0\n",
    "  for batch_idx, (frames, labels) in enumerate(train_dataloader):\n",
    "\n",
    "    frames, labels = frames.to(device), labels.to(device)\n",
    "\n",
    "    # reshaping features for model input\n",
    "    batch_size, num_frames, channels, height, width = frames.size()\n",
    "    reshaped_frames = frames.view(\n",
    "        batch_size * num_frames, channels, height, width)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(reshaped_frames)\n",
    "\n",
    "    # Reshape output to split batch and frame dimensions\n",
    "    reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "    probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "    # Aggregate predictions for each video\n",
    "    aggregated_probabilities = probabilities.mean(dim=1)\n",
    "    # Get the predicted class for each video\n",
    "    _, predicted_classes = torch.max(aggregated_probabilities, dim=1)\n",
    "\n",
    "    loss = criterion(aggregated_probabilities, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "\n",
    "  # Validation loop\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (frames, labels) in enumerate(test_dataloader):\n",
    "\n",
    "      # reshaping features for model input\n",
    "      batch_size, num_frames, channels, height, width = frames.size()\n",
    "      reshaped_frames = frames.view(\n",
    "          batch_size * num_frames, channels, height, width)\n",
    "\n",
    "      outputs = model(reshaped_frames)\n",
    "\n",
    "      # Reshape output to split batch and frame dimensions\n",
    "      reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "      probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "      # Aggregate predictions for each video\n",
    "      aggregated_probabilities = probabilities.mean(dim=1)\n",
    "      # Get the predicted class for each video\n",
    "      _, predicted_classes = torch.max(aggregated_probabilities, dim=1)\n",
    "\n",
    "      loss = criterion(aggregated_probabilities, labels)\n",
    "\n",
    "      val_loss += loss.item()\n",
    "      # _, predicted = torch.max(outputs, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted_classes == labels).sum().item()\n",
    "\n",
    "  # Calculate validation accuracy\n",
    "  val_loss /= len(test_dataloader.dataset)\n",
    "  val_accuracy = 100. * correct / total\n",
    "\n",
    "  # Print validation results\n",
    "  print(f\"Validation Results - Epoch {epoch+1}:\")\n",
    "  print(f\"Validation Loss: {val_loss:.4f} | Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "  # Save training and validation losses\n",
    "  train_losses.append(train_loss/len(train_dataloader.dataset))\n",
    "  val_losses.append(val_loss)\n",
    "\n",
    "  # Save model weights after every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    torch.save(model.state_dict(), model_weight_file)\n",
    "    print(f\"Epoch {epoch+1}: Model weights saved as {model_weight_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocess the video frames\n",
    "# def preprocess_frame(frame):\n",
    "#   frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "#   frame = transform(frame)\n",
    "#   return frame\n",
    "\n",
    "# # Function to classify video\n",
    "\n",
    "\n",
    "# def classify_video(video_path, threshold=0.5):\n",
    "#   cap = cv2.VideoCapture(video_path)\n",
    "#   frames = []\n",
    "#   while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#       break\n",
    "#     frame = preprocess_frame(frame)\n",
    "#     frames.append(frame)\n",
    "\n",
    "#   cap.release()\n",
    "\n",
    "#   # Convert frames to tensor and add batch dimension\n",
    "#   frames_tensor = torch.stack(frames).unsqueeze(0)\n",
    "\n",
    "#   # Pass frames through the model\n",
    "#   with torch.no_grad():\n",
    "#     model.eval()\n",
    "\n",
    "#     # Reshape frames for model input\n",
    "#     batch_size, num_frames, channels, height, width = frames_tensor.size()\n",
    "#     reshaped_frames = frames_tensor.view(\n",
    "#         batch_size * num_frames, channels, height, width)\n",
    "\n",
    "#     # Get model outputs\n",
    "#     outputs = model(reshaped_frames)\n",
    "\n",
    "#     # Reshape output to split batch and frame dimensions\n",
    "#     reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "#     probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "\n",
    "#     # Calculate the overall probability of violence across all frames\n",
    "#     overall_probability = probabilities[:, :, 1].mean()*100\n",
    "\n",
    "#     # Determine if the overall probability exceeds the threshold\n",
    "#     if overall_probability > threshold:\n",
    "#       prediction = \"Violence\"\n",
    "#     else:\n",
    "#       prediction = \"Non-violence\"\n",
    "\n",
    "#     return overall_probability.item(), prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the video frames\n",
    "# frames = [10, 3, 224, 224]\n",
    "def preprocess_frame(frames):\n",
    "  # num_frame = frames.shape[0]\n",
    "  frame_stack = []\n",
    "  for frame in frames:\n",
    "    frame = transform(frame)\n",
    "    frame_stack.append(frame)\n",
    "  return frame_stack\n",
    "\n",
    "# frame_stack = [10, 3, 224, 224]\n",
    "# Function to classify video\n",
    "\n",
    "\n",
    "def classify_video(frame_stack, threshold=0.5):\n",
    "\n",
    "  # Convert frames to tensor and add batch dimension\n",
    "  frames_tensor = torch.stack(frame_stack).unsqueeze(0)\n",
    "\n",
    "  # Pass frames through the model\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    # Reshape frames for model input\n",
    "    batch_size, num_frames, channels, height, width = frames_tensor.size()\n",
    "    reshaped_frames = frames_tensor.view(\n",
    "        batch_size * num_frames, channels, height, width)\n",
    "\n",
    "    # Get model outputs\n",
    "    outputs = model(reshaped_frames)\n",
    "\n",
    "    # Reshape output to split batch and frame dimensions\n",
    "    reshaped_output = outputs.view(batch_size, num_frames, -1)\n",
    "    probabilities = torch.softmax(reshaped_output, dim=2).float()\n",
    "\n",
    "    # Calculate the overall probability of violence across all frames\n",
    "    overall_probability = probabilities[:, :, 1].mean()*100\n",
    "\n",
    "    # Determine if the overall probability exceeds the threshold\n",
    "    if overall_probability > threshold:\n",
    "      prediction = \"Violence\"\n",
    "    else:\n",
    "      prediction = \"Non-violence\"\n",
    "\n",
    "    return overall_probability.item(), prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = r\"Dataset\\violent\\cam1\\47.mp4\"  # Path to your test video\n",
    "# result = classify_video(video_path)\n",
    "# print(\"Classification:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = np.load('arrays_data.npz')['arr1']\n",
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_frame(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_frame = preprocess_frame(frames)\n",
    "result = classify_video(preprocessed_frame)\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import websockets\n",
    "import asyncio\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "PORT = 8765\n",
    "print(\"server listening on port \" + str(PORT))\n",
    "\n",
    "\n",
    "async def main(websocket):\n",
    "  print(\"a client just connected\")\n",
    "  prev_message_time = None\n",
    "  pixel_data_store = []\n",
    "\n",
    "  global test_frames\n",
    "  try:\n",
    "    async for message in websocket:\n",
    "      current_time = time.time()\n",
    "      if prev_message_time is not None:\n",
    "        latency_ms = (current_time - prev_message_time) * 1000\n",
    "        print(\"Latency (ms) since previous message:\", latency_ms)\n",
    "      prev_message_time = current_time\n",
    "\n",
    "      pixel_data = np.frombuffer(message, dtype=np.uint8)\n",
    "\n",
    "      # pixel_data = pixel_data.reshape(240, 240, 4)[:, :, :3]\n",
    "\n",
    "      # pixel_data = np.array([\n",
    "      #     pixel_data[:, :, 0],\n",
    "      #     pixel_data[:, :, 1],\n",
    "      #     pixel_data[:, :, 2]\n",
    "      # ])\n",
    "\n",
    "      pixel_data_store.append(pixel_data)\n",
    "\n",
    "      if len(pixel_data_store) < 10:\n",
    "        continue\n",
    "\n",
    "      # 🤖\n",
    "      frames = np.array(pixel_data_store).reshape(10, 240, 240, 4)[:, :, :, :3]\n",
    "      # frames = np.array(pixel_data_store)\n",
    "      print(frames.shape)\n",
    "\n",
    "      np.savez('arrays_data.npz', arr1=frames)\n",
    "\n",
    "      # ⭐\n",
    "      preprocessed_frame = preprocess_frame(frames)\n",
    "      result = classify_video(preprocessed_frame)\n",
    "      print(str(result))\n",
    "\n",
    "      pixel_data_store = []\n",
    "  except websockets.exceptions.ConnectionClosed as e:\n",
    "    print(\"a client just disconnected\", e)\n",
    "\n",
    "\n",
    "start_server = websockets.serve(main, \"localhost\", PORT)\n",
    "asyncio.get_event_loop().run_until_complete(start_server)\n",
    "asyncio.get_event_loop().run_forever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
