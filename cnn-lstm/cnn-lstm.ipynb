{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture(filename, timesep, rgb, h, w):\n",
    "  frames = np.zeros((timesep, rgb, h, w), dtype=float)\n",
    "  i = 0\n",
    "  vc = cv2.VideoCapture(filename)\n",
    "  if vc.isOpened():\n",
    "    rval, frame = vc.read()\n",
    "    while rval and i < timesep:\n",
    "      # Resize the frame to the specified width and height\n",
    "      frm = cv2.resize(frame, (w, h))\n",
    "      frm = np.expand_dims(frm, axis=0)\n",
    "      frm = np.moveaxis(frm, -1, 1)\n",
    "      if np.max(frm) > 1:\n",
    "        frm = frm / 255.0\n",
    "      frames[i][:] = frm\n",
    "      i += 1\n",
    "      rval, frame = vc.read()\n",
    "    vc.release()  # Release the video capture object\n",
    "  else:\n",
    "    print(\"Error: Unable to open video file\")\n",
    "  return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeWarp(nn.Module):\n",
    "  '''video to frames'''\n",
    "\n",
    "  def __init__(self, baseModel, method='sqeeze'):\n",
    "    super(TimeWarp, self).__init__()\n",
    "    self.baseModel = baseModel\n",
    "    self.method = method\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size, time_steps, C, H, W = x.size()\n",
    "    if self.method == 'loop':\n",
    "      output = []\n",
    "      for i in range(time_steps):\n",
    "        # input one frame at a time into the basemodel\n",
    "        x_t = self.baseModel(x[:, i, :, :, :])\n",
    "        # Flatten the output\n",
    "        x_t = x_t.view(x_t.size(0), -1)\n",
    "        output.append(x_t)\n",
    "      # end loop\n",
    "      # make output as  ( samples, timesteps, output_size)\n",
    "      x = torch.stack(output, dim=0).transpose_(0, 1)\n",
    "      output = None  # clear var to reduce data  in memory\n",
    "      x_t = None  # clear var to reduce data  in memory\n",
    "    else:\n",
    "      # reshape input  to be (batch_size * timesteps, input_size)\n",
    "      x = x.contiguous().view(batch_size * time_steps, C, H, W)\n",
    "      x = self.baseModel(x)\n",
    "      x = x.view(x.size(0), -1)\n",
    "      # make output as  ( samples, timesteps, output_size)\n",
    "      x = x.contiguous().view(batch_size, time_steps, x.size(-1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class extractlastcell(nn.Module):\n",
    "  def forward(self, x):\n",
    "    out, _ = x\n",
    "    return out[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(weight='model_weight.pth'):\n",
    "  # Create model\n",
    "  num_classes = 1\n",
    "  dr_rate = 0.2\n",
    "  pretrained = True\n",
    "  rnn_hidden_size = 80\n",
    "  rnn_num_layers = 1\n",
    "  baseModel = models.vgg19(pretrained=pretrained).features\n",
    "\n",
    "  i = 0\n",
    "  for child in baseModel.children():\n",
    "    if i < 28:\n",
    "      for param in child.parameters():\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "      for param in child.parameters():\n",
    "        param.requires_grad = True\n",
    "    i += 1\n",
    "\n",
    "  num_features = 12800\n",
    "  # Example of using Sequential\n",
    "  model = nn.Sequential(\n",
    "      TimeWarp(baseModel, method='loop'),\n",
    "      nn.LSTM(num_features, rnn_hidden_size, rnn_num_layers,\n",
    "              batch_first=True, bidirectional=True),\n",
    "      extractlastcell(),\n",
    "      nn.Linear(160, 256),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(dr_rate),\n",
    "      nn.Linear(256, num_classes)\n",
    "  )\n",
    "  checkpoint = torch.load(weight, map_location=torch.device('cpu'))\n",
    "  model.load_state_dict(checkpoint['state_dict'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_fight(model, video, acuracy=0.58):\n",
    "  model.eval()\n",
    "  outputs = model(video)\n",
    "  torch.sigmoid(outputs)\n",
    "  preds = torch.sigmoid(outputs).to('cpu')\n",
    "  preds = preds.detach().numpy()\n",
    "  if preds[0][0] >= acuracy:\n",
    "    return True, preds[0][0]\n",
    "  else:\n",
    "    return False, preds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "device = torch.device(\"cpu\")   # Use CPU only\n",
    "model22 = Model()\n",
    "model22.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_fight(video_path, accuracyfight=0.65):\n",
    "  tmp_obj = {}\n",
    "\n",
    "  if os.path.exists('./tmp.mp4'):\n",
    "    os.remove('./tmp.mp4')\n",
    "\n",
    "  # Open the local video file\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "  # Check if the video file was opened successfully\n",
    "  if not cap.isOpened():\n",
    "    print(\"Error: Unable to open video file\")\n",
    "    return None\n",
    "\n",
    "  else:\n",
    "    # Open a file to write the video data\n",
    "    with open(\"tmp.mp4\", \"wb\") as file:\n",
    "      while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        # Check if the frame was read successfully\n",
    "        if not ret:\n",
    "          break  # Break the loop if no more frames are available\n",
    "        # Write the frame data to the file\n",
    "        file.write(frame)\n",
    "\n",
    "  vid = capture(\"tmp.mp4\", 40, 3, 170, 170)\n",
    "  v = np.array([vid])\n",
    "  v = torch.from_numpy(v)\n",
    "  v = v.to(device, dtype=torch.float)\n",
    "  millis = int(round(time.time() * 1000))\n",
    "  f, precent = pred_fight(model22, v, acuracy=accuracyfight)\n",
    "  tmp_obj = {'fight': f, 'precentegeoffight': str(precent)}\n",
    "  millis2 = int(round(time.time() * 1000))\n",
    "  tmp_obj['processing_time'] = str((millis2-millis)/1000)\n",
    "  return tmp_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_fight(video_path='give-your-video-path-to-predict-violence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Unable to open video file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fight': True, 'precentegeoffight': '0.66339463', 'processing_time': '5.582'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_fight(video_path=\"C:/Users/k25ha/OneDrive/Desktop/video_20240122_192851.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
